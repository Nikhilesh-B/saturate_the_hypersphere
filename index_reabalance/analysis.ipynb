{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSCI World Equity Index Inclusion Analysis\n",
    "\n",
    "This notebook analyzes the impact of stock returns following their inclusion in the MSCI World Equity Index.\n",
    "\n",
    "**Note:** A complete programmatic list of historical index changes typically requires a paid data subscription (e.g., Bloomberg, FactSet, MSCI). \n",
    "Below, we use a manually compiled sample of recent additions (2024-2025) and provide code to fetch their data using `yfinance`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install yfinance pandas matplotlib seaborn statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_msci_additions_from_csv(csv_path='msci_additions.csv'):\n",
    "    \"\"\"\n",
    "    Reads MSCI World Index additions from a CSV file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        # Parse dates\n",
    "        df['Announcement Date'] = pd.to_datetime(df['Announcement Date'])\n",
    "        df['Effective Date'] = pd.to_datetime(df['Effective Date'])\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {csv_path} not found. Please ensure the CSV exists.\")\n",
    "        return None\n",
    "\n",
    "additions_df = get_msci_additions_from_csv()\n",
    "if additions_df is not None:\n",
    "    print(\"Loaded additions data:\")\n",
    "    print(additions_df.head())\n",
    "    tickers = additions_df['Ticker'].tolist()\n",
    "    print(f\"\\nAnalyzing {len(tickers)} stocks: {tickers}\")\n",
    "else:\n",
    "    tickers = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data(tickers, period=\"5y\"):\n",
    "    data = {}\n",
    "    for ticker in tickers:\n",
    "        try:\n",
    "            # Fetch data\n",
    "            stock = yf.Ticker(ticker)\n",
    "            hist = stock.history(period=period)\n",
    "            if not hist.empty:\n",
    "                # Calculate daily returns\n",
    "                hist['Return'] = hist['Close'].pct_change()\n",
    "                data[ticker] = hist\n",
    "                print(f\"Fetched {ticker}\")\n",
    "            else:\n",
    "                print(f\"No data for {ticker}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {ticker}: {e}\")\n",
    "    return data\n",
    "\n",
    "stock_data = fetch_data(tickers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ff5_data(filepath='F-F_Research_Data_5_Factors_2x3_daily.csv'):\n",
    "    \"\"\"\n",
    "    Loads and processes Fama-French 5 Factor daily data.\n",
    "    Assumes the CSV has a date column (YYYYMMDD) and factor columns.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read CSV, skipping header rows if necessary (adjust 'skiprows' as needed)\n",
    "        # FF data usually has a few header lines\n",
    "        ff_data = pd.read_csv(filepath, skiprows=3)\n",
    "        \n",
    "        # Rename the first column to Date\n",
    "        ff_data.rename(columns={ff_data.columns[0]: 'Date'}, inplace=True)\n",
    "        \n",
    "        # Convert Date to datetime\n",
    "        ff_data['Date'] = pd.to_datetime(ff_data['Date'], format='%Y%m%d', errors='coerce')\n",
    "        ff_data = ff_data.dropna(subset=['Date'])\n",
    "        ff_data.set_index('Date', inplace=True)\n",
    "        \n",
    "        # Convert percentages to decimals (FF data is usually in percent)\n",
    "        ff_data = ff_data / 100.0\n",
    "        \n",
    "        # Rename factors for consistency\n",
    "        # Expected columns: Mkt-RF, SMB, HML, RMW, CMA, RF\n",
    "        ff_data.rename(columns={'Mkt-RF': 'Mkt_RF'}, inplace=True)\n",
    "        \n",
    "        print(\"Loaded Fama-French 5 Factor Data\")\n",
    "        return ff_data\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading FF5 data: {e}\")\n",
    "        return None\n",
    "\n",
    "ff5_data = load_ff5_data()\n",
    "if ff5_data is not None:\n",
    "    print(ff5_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_abnormal_returns_ff5(stock_data, ff5_data, additions_df, \n",
    "                                   estimation_window=250, \n",
    "                                   gap_window=30, \n",
    "                                   event_window=30):\n",
    "    \"\"\"\n",
    "    Calculates Abnormal Returns (AR) using the Fama-French 5 Factor Model.\n",
    "    \n",
    "    Methodology:\n",
    "    1. Estimation Window: [-250, -30] days relative to Announcement Date.\n",
    "    2. Estimate Betas (Mkt, SMB, HML, RMW, CMA) using OLS.\n",
    "    3. Calculate Expected Returns in the Event Window [-event_window, +event_window] relative to Effective Date.\n",
    "       (Or we can center around Announcement Date, but usually Effective Date is the trade target).\n",
    "       Let's center around Effective Date for the final visual, but we need to ensure the estimation window is clean (pre-announcement).\n",
    "    4. AR = Realized Return - Expected Return (Risk Free + Factor Loadings)\n",
    "    \"\"\"\n",
    "    \n",
    "    all_ar_data = []\n",
    "    \n",
    "    for index, row in additions_df.iterrows():\n",
    "        ticker = row['Ticker']\n",
    "        announce_date = row['Announcement Date']\n",
    "        effective_date = row['Effective Date']\n",
    "        \n",
    "        if ticker not in stock_data:\n",
    "            continue\n",
    "            \n",
    "        stock_df = stock_data[ticker].copy()\n",
    "        \n",
    "        # Align stock data with FF5 data\n",
    "        # Ensure timezone compatibility (remove tz from stock data if present, or localize FF)\n",
    "        stock_df.index = stock_df.index.tz_localize(None)\n",
    "        merged = stock_df[['Return']].join(ff5_data, how='inner')\n",
    "        merged['Excess_Return'] = merged['Return'] - merged['RF']\n",
    "        \n",
    "        if merged.empty:\n",
    "            continue\n",
    "\n",
    "        # --- Step 1: Estimation Window ---\n",
    "        # End estimation 'gap_window' days BEFORE Announcement to avoid information leakage\n",
    "        est_end_date = announce_date - timedelta(days=gap_window)\n",
    "        est_start_date = est_end_date - timedelta(days=estimation_window)\n",
    "        \n",
    "        estimation_data = merged.loc[est_start_date:est_end_date].dropna()\n",
    "        \n",
    "        if len(estimation_data) < 50: # Require at least 50 days of data for regression\n",
    "            # print(f\"Insufficient data for {ticker}\")\n",
    "            continue\n",
    "            \n",
    "        # FF5 Regression\n",
    "        X = estimation_data[['Mkt_RF', 'SMB', 'HML', 'RMW', 'CMA']]\n",
    "        X = sm.add_constant(X)\n",
    "        y = estimation_data['Excess_Return']\n",
    "        \n",
    "        try:\n",
    "            model = sm.OLS(y, X).fit()\n",
    "            # alphas = model.params['const'] # We usually assume alpha is 0 for expected returns or include it\n",
    "            # For standard event study, we use the estimated parameters to predict 'normal' return\n",
    "            params = model.params\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "        # --- Step 2: Event Window Calculation ---\n",
    "        # Define Event Window relative to Effective Date (t=0)\n",
    "        # We want data from [Effective - event_window] to [Effective + event_window]\n",
    "        evt_start = effective_date - timedelta(days=event_window*2) # Buffer for non-trading days\n",
    "        evt_end = effective_date + timedelta(days=event_window*2)\n",
    "        \n",
    "        event_data = merged.loc[evt_start:evt_end].copy()\n",
    "        \n",
    "        if event_data.empty:\n",
    "            continue\n",
    "            \n",
    "        # Calculate Expected Return using estimated betas\n",
    "        X_event = event_data[['Mkt_RF', 'SMB', 'HML', 'RMW', 'CMA']]\n",
    "        X_event = sm.add_constant(X_event)\n",
    "        \n",
    "        # If constant was dropped/not in index, ensure shape matches\n",
    "        if 'const' not in X_event.columns:\n",
    "             X_event['const'] = 1.0\n",
    "             \n",
    "        expected_excess_return = model.predict(X_event)\n",
    "        \n",
    "        # Abnormal Return (AR) = Actual Excess Return - Expected Excess Return\n",
    "        event_data['AR'] = event_data['Excess_Return'] - expected_excess_return\n",
    "        \n",
    "        # Create relative time index (t=0 is effective date)\n",
    "        # Find nearest date to effective date\n",
    "        dates = event_data.index\n",
    "        nearest_date = dates[np.argmin(np.abs(dates - effective_date))]\n",
    "        event_data['Event_Day'] = 0 # Placeholder\n",
    "        \n",
    "        # Assign event days simply by trading day count from nearest date\n",
    "        loc_zero = event_data.index.get_loc(nearest_date)\n",
    "        event_data['Event_Day'] = np.arange(len(event_data)) - loc_zero\n",
    "        \n",
    "        # Filter to strictly +/- event_window\n",
    "        event_data = event_data[(event_data['Event_Day'] >= -event_window) & \n",
    "                                (event_data['Event_Day'] <= event_window)]\n",
    "        \n",
    "        event_data['Ticker'] = ticker\n",
    "        all_ar_data.append(event_data[['Event_Day', 'AR', 'Ticker']])\n",
    "\n",
    "    if not all_ar_data:\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    return pd.concat(all_ar_data)\n",
    "\n",
    "ar_df = calculate_abnormal_returns_ff5(stock_data, ff5_data, additions_df, event_window=40)\n",
    "if not ar_df.empty:\n",
    "    print(\"Calculated FF5 Abnormal Returns\")\n",
    "    print(ar_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical Analysis of CAR (Cumulative Abnormal Returns)\n",
    "\n",
    "if not ar_df.empty:\n",
    "    # Pivot to have tickers as columns, Event_Day as index\n",
    "    ar_pivot = ar_df.pivot_table(index='Event_Day', columns='Ticker', values='AR')\n",
    "    \n",
    "    # 1. Average Abnormal Return (AAR) per day\n",
    "    aar = ar_pivot.mean(axis=1)\n",
    "    \n",
    "    # 2. Cumulative Average Abnormal Return (CAAR)\n",
    "    # We start cumulating from the beginning of the window (-40)\n",
    "    caar = aar.cumsum()\n",
    "    \n",
    "    # 3. T-Test Statistics\n",
    "    # Calculate standard deviation of ARs across the cross-section (tickers) for each day\n",
    "    std_ar = ar_pivot.std(axis=1)\n",
    "    n_stocks = ar_pivot.count(axis=1)\n",
    "    \n",
    "    # Standard error of AAR\n",
    "    # A simple test statistic: sqrt(N) * (AAR / StdDev_AR)\n",
    "    # (Note: There are more complex adjustments for serial correlation, but this is the standard base)\n",
    "    t_stats = np.sqrt(n_stocks) * (aar / std_ar)\n",
    "    \n",
    "    # Plotting CAAR\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(caar.index, caar.values, label='CAAR (FF5 Adjusted)', color='blue', linewidth=2)\n",
    "    \n",
    "    # Confidence Intervals (Approximate)\n",
    "    # For CAAR, variance accumulates. Variance(CAAR_t) = Sum(Variance(AAR_i))\n",
    "    # StdErr(CAAR_t) = sqrt(Sum(StdErr(AAR_i)^2))\n",
    "    # This is a simplification assuming independence over time.\n",
    "    \n",
    "    plt.title('Cumulative Average Abnormal Returns (CAAR) around Effective Date')\n",
    "    plt.xlabel('Days Relative to Effective Date (0)')\n",
    "    plt.ylabel('Cumulative Abnormal Return')\n",
    "    plt.axvline(x=0, color='red', linestyle='--', label='Effective Date')\n",
    "    plt.axhline(y=0, color='black', linewidth=1)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Identifying Optimal Windows\n",
    "    # Find the day with the minimum CAAR before t=0 (Potential Buy point)\n",
    "    pre_event_caar = caar[caar.index <= 0]\n",
    "    buy_signal_day = pre_event_caar.idxmin()\n",
    "    \n",
    "    # Find day with maximum CAAR (Potential Sell point)\n",
    "    # We look for max in the whole window or just post-announcement?\n",
    "    # Usually run-up happens pre-effective.\n",
    "    sell_signal_day = caar.idxmax()\n",
    "    \n",
    "    print(f\"Analysis Results:\")\n",
    "    print(f\"Lowest Cumulative Return Day (Potential Entry): Day {buy_signal_day} (CAAR: {pre_event_caar.min():.2%})\")\n",
    "    print(f\"Peak Cumulative Return Day (Potential Exit): Day {sell_signal_day} (CAAR: {caar.max():.2%})\")\n",
    "    \n",
    "    print(\"\\nDaily Statistics around Event:\")\n",
    "    stats_df = pd.DataFrame({'AAR': aar, 'CAAR': caar, 'T-Stat': t_stats})\n",
    "    # Show window from -10 to +10\n",
    "    print(stats_df.loc[-10:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
